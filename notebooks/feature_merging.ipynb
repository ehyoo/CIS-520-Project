{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the features and getting an 80/20 split.\n",
    "\n",
    "This has a circular dependency on the ngram_filtered_extraction notebook (whoops). Check out that notebook for running details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading in everything and making the full feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_dir = '../data/features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First loading in our CSVs\n",
    "\n",
    "def to_sparse_matrix(df):\n",
    "    return scipy.sparse.csc_matrix(df.values)\n",
    "\n",
    "pointedness_train_matr = to_sparse_matrix(pd.read_csv(os.path.join(data_base_dir, \"pointedness_train.csv\")))\n",
    "pointedness_test_matr = to_sparse_matrix(pd.read_csv(os.path.join(data_base_dir, \"pointedness_test.csv\")))\n",
    "\n",
    "synset_train_matr = to_sparse_matrix(pd.read_csv(os.path.join(data_base_dir, \"synset_train.csv\")))\n",
    "synset_test_matr = to_sparse_matrix(pd.read_csv(os.path.join(data_base_dir, \"synset_test.csv\")))\n",
    "\n",
    "frequency_train_matr = to_sparse_matrix(pd.read_csv(os.path.join(data_base_dir, 'frequency_train.csv')))\n",
    "frequency_test_matr = to_sparse_matrix(pd.read_csv(os.path.join(data_base_dir, 'frequency_test.csv')))\n",
    "\n",
    "sentiment_train_matr = to_sparse_matrix(pd.read_csv(os.path.join(data_base_dir, 'sentiment_train.csv')))\n",
    "sentiment_test_matr = to_sparse_matrix(pd.read_csv(os.path.join(data_base_dir, 'sentiment_test.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then loading in Artur's pattern work\n",
    "\n",
    "pattern_train = scipy.sparse.load_npz(os.path.join(data_base_dir, \"pattern_training.npz\"))\n",
    "pattern_test = scipy.sparse.load_npz(os.path.join(data_base_dir, \"pattern_test.npz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/samples/pol_train_cleaned.csv', sep='\\t')\n",
    "test_df = pd.read_csv('../data/samples/pol_test_cleaned.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = [\n",
    "    pointedness_train_matr,\n",
    "    synset_train_matr,\n",
    "    frequency_train_matr,\n",
    "    sentiment_train_matr,\n",
    "    pattern_train\n",
    "]\n",
    "\n",
    "testing_features = [\n",
    "    pointedness_test_matr,\n",
    "    synset_test_matr,\n",
    "    frequency_test_matr,\n",
    "    sentiment_test_matr,\n",
    "    pattern_test\n",
    "]\n",
    "\n",
    "X_train_full = scipy.sparse.hstack(training_features)\n",
    "X_test_full = scipy.sparse.hstack(testing_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1.1: Validating to ourselves that the output is what we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3049316, 491)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(764172, 491)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1.2: Writing our full data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz(open(os.path.join(data_base_dir, \"X_train_full.npz\"), \"wb+\"), X_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz(open(os.path.join(data_base_dir,\"X_test_full.npz\"), \"wb+\"), X_test_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Thinning things out to make a more balanced data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_full = train_df[['label']].values\n",
    "y_test_full = test_df[['label']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0078657])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train_full)/X_train_full.shape[0] # Roughly 0.7% of our data is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00774695])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test_full)/X_test_full.shape[0] # And same with our testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(15) # For deterministic output\n",
    "\n",
    "def get_more_balanced_data_set(X, y, df, prop_min=0.2):\n",
    "    '''\n",
    "    X: CSR Matrix of our features\n",
    "    y: Numpy array of our labels\n",
    "    prop_min: Float that represents what proportion of our data should be minority (at most)\n",
    "    \n",
    "    We're assuming the +1 class is positive.\n",
    "    '''\n",
    "    # Setting up how many negative classifications needed\n",
    "    num_positive = sum(y)[0]\n",
    "    num_total = X.shape[0]\n",
    "    num_majority_needed = (1-prop_min) * (num_positive/prop_min)\n",
    "    \n",
    "    # Getting our positively classified values\n",
    "    positive_indices = (y==1).flatten()\n",
    "    \n",
    "    # Getting our negatively classified values\n",
    "    negative_indices = (y==0).flatten()\n",
    "    # Then randomly removing rows until we get how many we need\n",
    "    frac_needed = num_majority_needed/(num_total * 1.0)\n",
    "    for i in range(len(negative_indices)):\n",
    "        uniform_distribution_draw = np.random.uniform()\n",
    "        if uniform_distribution_draw > frac_needed:\n",
    "            negative_indices[i] = False\n",
    "    \n",
    "    # Merging the two together\n",
    "    indices_desired = np.logical_or(positive_indices, negative_indices)\n",
    "    \n",
    "    # Extracting rows then returning.\n",
    "    return (X[indices_desired, ], y[indices_desired], df[indices_desired])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_full_csr = X_test_full.tocsr()\n",
    "X_train_full_csr = X_train_full.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_balanced, y_test_balanced, test_df_balanced = get_more_balanced_data_set(X_test_full_csr, y_test_full, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_balanced, y_train_balanced, train_df_balanced = get_more_balanced_data_set(X_train_full_csr, y_train_full, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119293, 491)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then to validate to ourselves this worked\n",
    "X_train_balanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29526, 491)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_balanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119293"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29526"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20105958])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train_balanced)/len(y_train_balanced) # Just about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20050125])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test_balanced)/len(y_test_balanced) # This too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then saving everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, a last minute addition:\n",
    "\n",
    "ngram_all_train = scipy.sparse.load_npz(os.path.join(data_base_dir, \"ngram_train_balanced.npz\"))\n",
    "ngram_all_test = scipy.sparse.load_npz(os.path.join(data_base_dir, \"ngram_test_balanced.npz\"))\n",
    "\n",
    "ngram_words_only_train = scipy.sparse.load_npz(os.path.join(data_base_dir, \"ngram_train_words_only_balanced.npz\"))\n",
    "ngram_words_only_test = scipy.sparse.load_npz(os.path.join(data_base_dir, \"ngram_test_words_only_balanced.npz\"))\n",
    "\n",
    "ngram_pos_only_train = scipy.sparse.load_npz(os.path.join(data_base_dir, \"ngram_train_pos_only_balanced.npz\"))\n",
    "ngram_pos_only_test = scipy.sparse.load_npz(os.path.join(data_base_dir, \"ngram_test_pos_only_balanced.npz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_balanced_all = scipy.sparse.hstack([X_train_balanced, ngram_all_train])\n",
    "X_test_balanced_all = scipy.sparse.hstack([X_test_balanced, ngram_all_test])\n",
    "\n",
    "X_train_balanced_words_only = scipy.sparse.hstack([X_train_balanced, ngram_words_only_train])\n",
    "X_test_balanced_words_only = scipy.sparse.hstack([X_test_balanced, ngram_words_only_test])\n",
    "\n",
    "X_train_balanced_pos_only = scipy.sparse.hstack([X_train_balanced, ngram_pos_only_train])\n",
    "X_test_balanced_pos_only = scipy.sparse.hstack([X_test_balanced, ngram_pos_only_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119293, 20319)\n",
      "(119293, 2039)\n",
      "(119293, 18771)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_balanced_all.shape)\n",
    "print(X_train_balanced_words_only.shape)\n",
    "print(X_train_balanced_pos_only.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz(open(os.path.join(data_base_dir, \"X_train_balanced_all.npz\"), \"wb+\"), X_train_balanced_all.tocoo())\n",
    "scipy.sparse.save_npz(open(os.path.join(data_base_dir,\"X_test_balanced_all.npz\"), \"wb+\"), X_test_balanced_all.tocoo())\n",
    "\n",
    "scipy.sparse.save_npz(open(os.path.join(data_base_dir, \"X_train_words_only_balanced.npz\"), \"wb+\"), X_train_balanced_words_only.tocoo())\n",
    "scipy.sparse.save_npz(open(os.path.join(data_base_dir,\"X_test_words_only_balanced.npz\"), \"wb+\"), X_test_balanced_words_only.tocoo())\n",
    "\n",
    "scipy.sparse.save_npz(open(os.path.join(data_base_dir, \"X_train_pos_only_balanced.npz\"), \"wb+\"), X_train_balanced_pos_only.tocoo())\n",
    "scipy.sparse.save_npz(open(os.path.join(data_base_dir,\"X_test_pos_only_balanced.npz\"), \"wb+\"), X_test_balanced_pos_only.tocoo())\n",
    "\n",
    "scipy.sparse.save_npz(open(os.path.join(data_base_dir, \"X_train_no_ngram.npz\"), \"wb+\"), X_train_balanced.tocoo())\n",
    "scipy.sparse.save_npz(open(os.path.join(data_base_dir,\"X_test_no_ngram.npz\"), \"wb+\"), X_test_balanced.tocoo())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(open(os.path.join(data_base_dir, \"y_train_balanced.npy\"), \"wb+\"), arr=y_train_balanced)\n",
    "np.save(open(os.path.join(data_base_dir,\"y_test_balanced.npy\"), \"wb+\"), arr=y_test_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_balanced.to_csv('../data/samples/pol_train_rebalanced_cleaned.csv', sep='\\t', index=False)\n",
    "test_df_balanced.to_csv('../data/samples/pol_test_rebalanced_cleaned.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, getting our feature names.\n",
    "pointedness_df = pd.read_csv(os.path.join(data_base_dir, \"pointedness_train.csv\"))\n",
    "pointedness_cols = [x for x in pointedness_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "synset_df = pd.read_csv(os.path.join(data_base_dir, \"synset_train.csv\"))\n",
    "synset_cols = [x for x in synset_df.columns]\n",
    "\n",
    "frequency_df = pd.read_csv(os.path.join(data_base_dir, 'frequency_train.csv'))\n",
    "frequency_cols = [x for x in frequency_df.columns]\n",
    "\n",
    "sentiment_df = pd.read_csv(os.path.join(data_base_dir, 'sentiment_train.csv'))\n",
    "sentiment_cols = [x for x in sentiment_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_cols_raw = json.load(open(os.path.join(data_base_dir, 'pattern_features_names.json'), 'r'))\n",
    "pattern_cols = [str(x) for x in pattern_cols_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['O', '101', ['in', 'a']]\",\n",
       " \"['O', '101', ['i', 'been']]\",\n",
       " \"['O', '1001', ['i', 'for']]\",\n",
       " \"['O', '101', ['they', 'it']]\",\n",
       " \"['O', '101', ['and', 'they']]\",\n",
       " \"['O', '1001', ['in', 'the']]\",\n",
       " \"['O', '101', ['could', 'a']]\",\n",
       " \"['O', '101', ['we', 'had']]\",\n",
       " '[\\'O\\', \\'101\\', [\"\\'m\", \\'he\\']]',\n",
       " '[\\'O\\', \\'1101\\', [\\'i\\', \"\\'m\", \\'he\\']]',\n",
       " \"['O', '101', ['of', 'you']]\",\n",
       " \"['O', '1101', ['are', 'you', 'about']]\",\n",
       " \"['O', '101', ['you', 'the']]\",\n",
       " \"['O', '101', ['a', 'more']]\",\n",
       " \"['O', '101', ['is', 'going']]\",\n",
       " \"['O', '10001', ['is', 'and']]\",\n",
       " \"['O', '1101', ['in', 'the', 'that']]\",\n",
       " \"['O', '101', ['not', 'about']]\",\n",
       " \"['O', '1101', ['would', 'be', 'to']]\",\n",
       " \"['O', '1001', ['i', 'in']]\",\n",
       " \"['O', '101', ['that', 'him']]\",\n",
       " \"['O', '101', ['and', 'him']]\",\n",
       " '[\\'O\\', \\'1101\\', [\\'you\\', \"\\'re\", \\'the\\']]',\n",
       " \"['O', '1101', ['have', 'a', 'of']]\",\n",
       " \"['O', '101', ['your', 'is']]\",\n",
       " \"['O', '1001', ['i', 'that']]\",\n",
       " \"['O', '1001', ['i', 'to']]\",\n",
       " '[\\'O\\', \\'101\\', [\"n\\'t\", \\'me\\']]',\n",
       " \"['O', '101', ['your', 'on']]\",\n",
       " \"['O', '1101', ['of', 'a', 'of']]\",\n",
       " \"['O', '101', ['is', 'from']]\",\n",
       " \"['O', '101', ['i', 'like']]\",\n",
       " \"['O', '1001', ['the', 'as']]\",\n",
       " \"['O', '10001', ['the', 'on']]\",\n",
       " \"['O', '101', ['my', 'is']]\",\n",
       " \"['O', '1001', ['the', 'about']]\",\n",
       " \"['O', '1101', ['are', 'you', 'to']]\",\n",
       " \"['O', '101', ['i', 'be']]\",\n",
       " \"['O', '101', ['obama', 'the']]\",\n",
       " \"['O', '101', ['get', 'for']]\",\n",
       " \"['O', '101', ['the', 'but']]\",\n",
       " \"['O', '1101', ['it', 'has', 'to']]\",\n",
       " \"['O', '1101', ['for', 'the', 'and']]\",\n",
       " \"['O', '101', ['the', 'if']]\",\n",
       " \"['O', '101', ['so', 'the']]\",\n",
       " \"['O', '101', ['an', 'for']]\",\n",
       " \"['O', '101', ['i', 'say']]\",\n",
       " '[\\'O\\', \\'101\\', [\"\\'m\", \\'she\\']]',\n",
       " '[\\'O\\', \\'1101\\', [\\'i\\', \"\\'m\", \\'she\\']]',\n",
       " \"['O', '1001', ['i', 'it']]\",\n",
       " \"['O', '1001', ['a', 'the']]\",\n",
       " \"['O', '101', ['same', 'as']]\",\n",
       " \"['O', '101', ['obama', 'to']]\",\n",
       " \"['O', '1101', ['going', 'to', 'up']]\",\n",
       " \"['O', '101', ['your', 'to']]\",\n",
       " \"['O', '101', ['not', 'and']]\",\n",
       " \"['O', '101', ['then', 'to']]\",\n",
       " \"['O', '101', ['the', 'into']]\",\n",
       " \"['O', '101', ['all', 'people']]\",\n",
       " \"['O', '101', ['even', 'to']]\",\n",
       " \"['O', '101', ['the', 'a']]\",\n",
       " \"['O', '1101', ['about', 'the', 'and']]\",\n",
       " \"['O', '101', ['are', 'out']]\",\n",
       " \"['O', '101', ['because', 'people']]\",\n",
       " \"['O', '101', ['i', 'not']]\",\n",
       " \"['O', '101', ['i', 'said']]\",\n",
       " \"['O', '101', ['his', 'is']]\",\n",
       " \"['O', '1001', ['those', 'are']]\",\n",
       " '[\\'O\\', \\'101\\', [\"\\'m\", \\'it\\']]',\n",
       " '[\\'O\\', \\'1101\\', [\\'i\\', \"\\'m\", \\'it\\']]',\n",
       " '[\\'O\\', \\'1101\\', [\\'it\\', \"\\'s\", \\'and\\']]',\n",
       " \"['O', '101', ['it', 'in']]\",\n",
       " \"['O', '101', ['have', 'this']]\",\n",
       " \"['O', '101', ['you', 'think']]\",\n",
       " '[\\'O\\', \\'101\\', [\"\\'s\", \\'an\\']]',\n",
       " \"['O', '1001', ['you', 'a']]\",\n",
       " \"['O', '101', ['being', 'for']]\",\n",
       " \"['O', '101', ['i', 'him']]\",\n",
       " \"['O', '101', ['you', 'your']]\",\n",
       " \"['O', '1001', ['not', 'to']]\",\n",
       " \"['O', '1001', ['the', 'than']]\",\n",
       " '[\\'O\\', \\'101\\', [\"\\'m\", \\'about\\']]',\n",
       " '[\\'O\\', \\'1101\\', [\\'i\\', \"\\'m\", \\'about\\']]',\n",
       " \"['O', '101', ['good', 'of']]\",\n",
       " \"['O', '1101', ['a', 'good', 'of']]\",\n",
       " '[\\'O\\', \\'1101\\', [\\'it\\', \"\\'s\", \\'a\\']]',\n",
       " \"['O', '101', ['no', 'that']]\",\n",
       " \"['O', '101', ['your', 'are']]\",\n",
       " \"['O', '101', ['at', 'a']]\",\n",
       " \"['O', '101', ['but', 'is']]\",\n",
       " \"['O', '1101', ['is', 'more', 'than']]\",\n",
       " '[\\'O\\', \\'1101\\', [\\'you\\', \"\\'re\", \\'about\\']]',\n",
       " \"['O', '101', ['if', 'is']]\",\n",
       " '[\\'O\\', \\'101\\', [\"\\'s\", \\'what\\']]',\n",
       " \"['O', '101', ['i', 'with']]\",\n",
       " \"['O', '101', ['so', 'to']]\",\n",
       " \"['O', '101', ['because', 'and']]\",\n",
       " \"['O', '1101', ['of', 'the', 'people']]\",\n",
       " \"['O', '101', ['because', 'is']]\",\n",
       " \"['O', '101', ['those', 'people']]\",\n",
       " \"['O', '101', ['was', 'that']]\",\n",
       " \"['O', '101', ['to', 'us']]\",\n",
       " \"['O', '101', ['you', 'in']]\",\n",
       " \"['O', '1101', ['there', 'are', 'of']]\",\n",
       " \"['O', '101', ['what', 'you']]\",\n",
       " \"['O', '101', ['a', 'but']]\",\n",
       " \"['O', '101', ['of', 'not']]\",\n",
       " \"['O', '101', ['not', 'it']]\",\n",
       " \"['O', '101', ['but', 'to']]\",\n",
       " \"['O', '11001', ['have', 'a', 'of']]\",\n",
       " \"['O', '101', ['so', 'and']]\",\n",
       " \"['O', '101', ['i', 'do']]\",\n",
       " \"['O', '101', ['not', 'on']]\",\n",
       " \"['O', '1101', ['is', 'not', 'to']]\",\n",
       " \"['O', '101', ['do', 'that']]\",\n",
       " \"['O', '101', ['the', 'you']]\",\n",
       " '[\\'O\\', \\'1101\\', [\\'does\\', \"n\\'t\", \\'to\\']]',\n",
       " \"['O', '1001', ['i', 'with']]\",\n",
       " '[\\'O\\', \\'1101\\', [\\'does\\', \"n\\'t\", \\'that\\']]',\n",
       " \"['O', '101', ['and', 'like']]\",\n",
       " \"['O', '101', ['you', 'up']]\",\n",
       " \"['O', '101', ['be', 'out']]\",\n",
       " \"['O', '1101', ['have', 'no', 'what']]\",\n",
       " \"['O', '101', ['is', 'as']]\",\n",
       " '[\\'O\\', \\'101\\', [\"\\'m\", \\'that\\']]',\n",
       " '[\\'O\\', \\'1101\\', [\\'i\\', \"\\'m\", \\'that\\']]',\n",
       " \"['O', '101', ['only', 'people']]\",\n",
       " '[\\'O\\', \\'101\\', [\"\\'m\", \\'they\\']]',\n",
       " '[\\'O\\', \\'1101\\', [\\'i\\', \"\\'m\", \\'they\\']]',\n",
       " \"['O', '101', ['only', 'if']]\",\n",
       " '[\\'O\\', \\'101\\', [\"\\'re\", \\'with\\']]',\n",
       " \"['O', '1101', ['to', 'get', 'of']]\",\n",
       " \"['O', '101', ['money', 'to']]\",\n",
       " \"['O', '1101', ['i', 'was', 'to']]\",\n",
       " \"['O', '1101', ['not', 'a', 'of']]\",\n",
       " \"['O', '101', ['no', 'at']]\",\n",
       " '[\\'O\\', \\'101\\', [\"n\\'t\", \\'him\\']]',\n",
       " '[\\'O\\', \\'101\\', [\"\\'m\", \\'the\\']]',\n",
       " '[\\'O\\', \\'1101\\', [\\'i\\', \"\\'m\", \\'the\\']]',\n",
       " \"['O', '101', ['those', 'and']]\",\n",
       " \"['O', '1101', ['think', 'you', 'the']]\",\n",
       " \"['O', '101', ['you', 'this']]\",\n",
       " \"['O', '1101', ['the', 'same', 'as']]\",\n",
       " \"['O', '101', ['still', 'to']]\",\n",
       " \"['O', '101', ['go', 'the']]\",\n",
       " \"['O', '101', ['he', 'on']]\",\n",
       " \"['O', '101', ['of', 'who']]\",\n",
       " \"['O', '101', ['or', 'or']]\",\n",
       " \"['O', '1001', ['because', 'is']]\",\n",
       " \"['O', '101', ['also', 'the']]\",\n",
       " \"['O', '1001', ['a', 'if']]\",\n",
       " \"['O', '1101', ['as', 'a', 'of']]\",\n",
       " \"['O', '101', ['that', 'that']]\",\n",
       " \"['O', '101', ['what', 'when']]\",\n",
       " \"['O', '1001', ['a', 'but']]\",\n",
       " \"['O', '101', ['the', 'than']]\",\n",
       " \"['O', '1101', ['all', 'the', 'and']]\",\n",
       " \"['O', '101', ['who', 'about']]\",\n",
       " \"['O', '101', ['i', 'think']]\",\n",
       " \"['O', '101', ['hillary', 'the']]\",\n",
       " \"['O', '101', ['but', 'are']]\",\n",
       " \"['O', '1001', ['but', 'are']]\",\n",
       " \"['O', '101', ['a', 'so']]\",\n",
       " \"['O', '101', ['would', 'be']]\",\n",
       " \"['O', '1001', ['to', 'up']]\",\n",
       " \"['O', '101', ['what', 'do']]\",\n",
       " \"['O', '101', ['more', 'for']]\",\n",
       " \"['O', '101', ['we', 'them']]\",\n",
       " \"['O', '1001', ['most', 'in']]\",\n",
       " \"['O', '11001', ['the', 'most', 'in']]\",\n",
       " '[\\'O\\', \\'101\\', [\\'you\\', \"n\\'t\"]]',\n",
       " \"['O', '101', ['not', 'what']]\",\n",
       " \"['O', '101', ['at', 'he']]\",\n",
       " \"['O', '101', ['a', 'a']]\",\n",
       " \"['O', '101', ['just', 'about']]\",\n",
       " '[\\'O\\', \\'101\\', [\\'so\\', \"\\'s\"]]',\n",
       " \"['O', '101', ['it', 'you']]\",\n",
       " '[\\'O\\', \\'101\\', [\"n\\'t\", \\'your\\']]',\n",
       " \"['O', '101', ['not', 'you']]\",\n",
       " \"['O', '101', ['just', 'her']]\",\n",
       " \"['O', '1101', ['of', 'the', 'party']]\",\n",
       " \"['O', '101', ['but', 'it']]\",\n",
       " \"['O', '1001', ['to', 'at']]\",\n",
       " \"['O', '101', ['any', 'now']]\",\n",
       " \"['O', '101', ['who', 'with']]\",\n",
       " \"['O', '101', ['be', 'or']]\",\n",
       " \"['O', '101', ['good', 'to']]\",\n",
       " \"['O', '101', ['to', 'those']]\",\n",
       " '[\\'O\\', \\'1101\\', [\\'you\\', \"\\'re\", \\'to\\']]',\n",
       " \"['O', '101', ['same', 'of']]\",\n",
       " \"['O', '1101', ['the', 'same', 'of']]\",\n",
       " '[\\'O\\', \\'101\\', [\"\\'ll\", \\'you\\']]',\n",
       " '[\\'O\\', \\'1101\\', [\\'i\\', \"\\'ll\", \\'you\\']]',\n",
       " \"['O', '101', ['same', 'that']]\",\n",
       " \"['O', '1101', ['the', 'same', 'that']]\",\n",
       " \"['O', '101', ['i', 'up']]\",\n",
       " \"['O', '101', ['of', 'i']]\",\n",
       " '[\\'O\\', \\'101\\', [\"\\'m\", \\'of\\']]',\n",
       " '[\\'O\\', \\'1101\\', [\\'i\\', \"\\'m\", \\'of\\']]',\n",
       " \"['O', '101', ['just', 'your']]\",\n",
       " \"['O', '1101', ['and', 'the', 'to']]\",\n",
       " '[\\'O\\', \\'101\\', [\"\\'re\", \\'and\\']]',\n",
       " \"['O', '101', ['a', 'good']]\",\n",
       " \"['O', '1001', ['your', 'is']]\",\n",
       " \"['O', '101', ['will', 'all']]\",\n",
       " \"['O', '101', ['how', 'you']]\",\n",
       " \"['O', '101', ['as', 'who']]\",\n",
       " \"['O', '101', ['our', 'in']]\",\n",
       " '[\\'O\\', \\'101\\', [\"\\'m\", \\'this\\']]',\n",
       " '[\\'O\\', \\'1101\\', [\\'i\\', \"\\'m\", \\'this\\']]',\n",
       " \"['O', '101', ['i', 'your']]\",\n",
       " \"['O', '101', ['is', 'up']]\",\n",
       " '[\\'O\\', \\'1101\\', [\\'that\\', \"\\'s\", \\'what\\']]',\n",
       " \"['O', '101', ['would', 'it']]\",\n",
       " '[\\'O\\', \\'101\\', [\"\\'s\", \\'because\\']]',\n",
       " \"['O', '1001', ['our', 'and']]\",\n",
       " \"['O', '101', ['actually', 'to']]\",\n",
       " \"['O', '1001', ['a', 'because']]\",\n",
       " '[\\'O\\', \\'101\\', [\"\\'s\", \\'this\\']]',\n",
       " \"['O', '101', ['or', 'it']]\",\n",
       " \"['O', '101', ['to', 'who']]\",\n",
       " \"['O', '101', ['so', 'they']]\",\n",
       " \"['O', '1001', ['was', 'and']]\",\n",
       " \"['O', '101', ['from', 'the']]\",\n",
       " \"['O', '101', ['and', 'our']]\",\n",
       " \"['O', '1101', ['so', 'it', 'be']]\",\n",
       " \"['O', '101', ['what', 'are']]\",\n",
       " '[\\'O\\', \\'1101\\', [\\'it\\', \"\\'s\", \\'because\\']]',\n",
       " \"['O', '101', ['need', 'to']]\",\n",
       " \"['O', '101', ['i', 'obama']]\",\n",
       " \"['O', '101', ['will', 'us']]\",\n",
       " \"['O', '101', ['because', 'are']]\",\n",
       " \"['O', '10001', ['i', 'that']]\",\n",
       " \"['O', '101', ['of', 'we']]\",\n",
       " \"['P', '101', ['in', 'a']]\",\n",
       " \"['P', '101', ['i', 'been']]\",\n",
       " \"['P', '1001', ['i', 'for']]\",\n",
       " \"['P', '101', ['they', 'it']]\",\n",
       " \"['P', '101', ['and', 'they']]\",\n",
       " \"['P', '1001', ['in', 'the']]\",\n",
       " \"['P', '101', ['could', 'a']]\",\n",
       " \"['P', '101', ['we', 'had']]\",\n",
       " '[\\'P\\', \\'101\\', [\"\\'m\", \\'he\\']]',\n",
       " '[\\'P\\', \\'1101\\', [\\'i\\', \"\\'m\", \\'he\\']]',\n",
       " \"['P', '101', ['of', 'you']]\",\n",
       " \"['P', '1101', ['are', 'you', 'about']]\",\n",
       " \"['P', '101', ['you', 'the']]\",\n",
       " \"['P', '101', ['a', 'more']]\",\n",
       " \"['P', '101', ['is', 'going']]\",\n",
       " \"['P', '10001', ['is', 'and']]\",\n",
       " \"['P', '1101', ['in', 'the', 'that']]\",\n",
       " \"['P', '101', ['not', 'about']]\",\n",
       " \"['P', '1101', ['would', 'be', 'to']]\",\n",
       " \"['P', '1001', ['i', 'in']]\",\n",
       " \"['P', '101', ['that', 'him']]\",\n",
       " \"['P', '101', ['and', 'him']]\",\n",
       " '[\\'P\\', \\'1101\\', [\\'you\\', \"\\'re\", \\'the\\']]',\n",
       " \"['P', '1101', ['have', 'a', 'of']]\",\n",
       " \"['P', '101', ['your', 'is']]\",\n",
       " \"['P', '1001', ['i', 'that']]\",\n",
       " \"['P', '1001', ['i', 'to']]\",\n",
       " '[\\'P\\', \\'101\\', [\"n\\'t\", \\'me\\']]',\n",
       " \"['P', '101', ['your', 'on']]\",\n",
       " \"['P', '1101', ['of', 'a', 'of']]\",\n",
       " \"['P', '101', ['is', 'from']]\",\n",
       " \"['P', '101', ['i', 'like']]\",\n",
       " \"['P', '1001', ['the', 'as']]\",\n",
       " \"['P', '10001', ['the', 'on']]\",\n",
       " \"['P', '101', ['my', 'is']]\",\n",
       " \"['P', '1001', ['the', 'about']]\",\n",
       " \"['P', '1101', ['are', 'you', 'to']]\",\n",
       " \"['P', '101', ['i', 'be']]\",\n",
       " \"['P', '101', ['obama', 'the']]\",\n",
       " \"['P', '101', ['get', 'for']]\",\n",
       " \"['P', '101', ['the', 'but']]\",\n",
       " \"['P', '1101', ['it', 'has', 'to']]\",\n",
       " \"['P', '1101', ['for', 'the', 'and']]\",\n",
       " \"['P', '101', ['the', 'if']]\",\n",
       " \"['P', '101', ['so', 'the']]\",\n",
       " \"['P', '101', ['an', 'for']]\",\n",
       " \"['P', '101', ['i', 'say']]\",\n",
       " '[\\'P\\', \\'101\\', [\"\\'m\", \\'she\\']]',\n",
       " '[\\'P\\', \\'1101\\', [\\'i\\', \"\\'m\", \\'she\\']]',\n",
       " \"['P', '1001', ['i', 'it']]\",\n",
       " \"['P', '1001', ['a', 'the']]\",\n",
       " \"['P', '101', ['same', 'as']]\",\n",
       " \"['P', '101', ['obama', 'to']]\",\n",
       " \"['P', '1101', ['going', 'to', 'up']]\",\n",
       " \"['P', '101', ['your', 'to']]\",\n",
       " \"['P', '101', ['not', 'and']]\",\n",
       " \"['P', '101', ['then', 'to']]\",\n",
       " \"['P', '101', ['the', 'into']]\",\n",
       " \"['P', '101', ['all', 'people']]\",\n",
       " \"['P', '101', ['even', 'to']]\",\n",
       " \"['P', '101', ['the', 'a']]\",\n",
       " \"['P', '1101', ['about', 'the', 'and']]\",\n",
       " \"['P', '101', ['are', 'out']]\",\n",
       " \"['P', '101', ['because', 'people']]\",\n",
       " \"['P', '101', ['i', 'not']]\",\n",
       " \"['P', '101', ['i', 'said']]\",\n",
       " \"['P', '101', ['his', 'is']]\",\n",
       " \"['P', '1001', ['those', 'are']]\",\n",
       " '[\\'P\\', \\'101\\', [\"\\'m\", \\'it\\']]',\n",
       " '[\\'P\\', \\'1101\\', [\\'i\\', \"\\'m\", \\'it\\']]',\n",
       " '[\\'P\\', \\'1101\\', [\\'it\\', \"\\'s\", \\'and\\']]',\n",
       " \"['P', '101', ['it', 'in']]\",\n",
       " \"['P', '101', ['have', 'this']]\",\n",
       " \"['P', '101', ['you', 'think']]\",\n",
       " '[\\'P\\', \\'101\\', [\"\\'s\", \\'an\\']]',\n",
       " \"['P', '1001', ['you', 'a']]\",\n",
       " \"['P', '101', ['being', 'for']]\",\n",
       " \"['P', '101', ['i', 'him']]\",\n",
       " \"['P', '101', ['you', 'your']]\",\n",
       " \"['P', '1001', ['not', 'to']]\",\n",
       " \"['P', '1001', ['the', 'than']]\",\n",
       " '[\\'P\\', \\'101\\', [\"\\'m\", \\'about\\']]',\n",
       " '[\\'P\\', \\'1101\\', [\\'i\\', \"\\'m\", \\'about\\']]',\n",
       " \"['P', '101', ['good', 'of']]\",\n",
       " \"['P', '1101', ['a', 'good', 'of']]\",\n",
       " '[\\'P\\', \\'1101\\', [\\'it\\', \"\\'s\", \\'a\\']]',\n",
       " \"['P', '101', ['no', 'that']]\",\n",
       " \"['P', '101', ['your', 'are']]\",\n",
       " \"['P', '101', ['at', 'a']]\",\n",
       " \"['P', '101', ['but', 'is']]\",\n",
       " \"['P', '1101', ['is', 'more', 'than']]\",\n",
       " '[\\'P\\', \\'1101\\', [\\'you\\', \"\\'re\", \\'about\\']]',\n",
       " \"['P', '101', ['if', 'is']]\",\n",
       " '[\\'P\\', \\'101\\', [\"\\'s\", \\'what\\']]',\n",
       " \"['P', '101', ['i', 'with']]\",\n",
       " \"['P', '101', ['so', 'to']]\",\n",
       " \"['P', '101', ['because', 'and']]\",\n",
       " \"['P', '1101', ['of', 'the', 'people']]\",\n",
       " \"['P', '101', ['because', 'is']]\",\n",
       " \"['P', '101', ['those', 'people']]\",\n",
       " \"['P', '101', ['was', 'that']]\",\n",
       " \"['P', '101', ['to', 'us']]\",\n",
       " \"['P', '101', ['you', 'in']]\",\n",
       " \"['P', '1101', ['there', 'are', 'of']]\",\n",
       " \"['P', '101', ['what', 'you']]\",\n",
       " \"['P', '101', ['a', 'but']]\",\n",
       " \"['P', '101', ['of', 'not']]\",\n",
       " \"['P', '101', ['not', 'it']]\",\n",
       " \"['P', '101', ['but', 'to']]\",\n",
       " \"['P', '11001', ['have', 'a', 'of']]\",\n",
       " \"['P', '101', ['so', 'and']]\",\n",
       " \"['P', '101', ['i', 'do']]\",\n",
       " \"['P', '101', ['not', 'on']]\",\n",
       " \"['P', '1101', ['is', 'not', 'to']]\",\n",
       " \"['P', '101', ['do', 'that']]\",\n",
       " \"['P', '101', ['the', 'you']]\",\n",
       " '[\\'P\\', \\'1101\\', [\\'does\\', \"n\\'t\", \\'to\\']]',\n",
       " \"['P', '1001', ['i', 'with']]\",\n",
       " '[\\'P\\', \\'1101\\', [\\'does\\', \"n\\'t\", \\'that\\']]',\n",
       " \"['P', '101', ['and', 'like']]\",\n",
       " \"['P', '101', ['you', 'up']]\",\n",
       " \"['P', '101', ['be', 'out']]\",\n",
       " \"['P', '1101', ['have', 'no', 'what']]\",\n",
       " \"['P', '101', ['is', 'as']]\",\n",
       " '[\\'P\\', \\'101\\', [\"\\'m\", \\'that\\']]',\n",
       " '[\\'P\\', \\'1101\\', [\\'i\\', \"\\'m\", \\'that\\']]',\n",
       " \"['P', '101', ['only', 'people']]\",\n",
       " '[\\'P\\', \\'101\\', [\"\\'m\", \\'they\\']]',\n",
       " '[\\'P\\', \\'1101\\', [\\'i\\', \"\\'m\", \\'they\\']]',\n",
       " \"['P', '101', ['only', 'if']]\",\n",
       " '[\\'P\\', \\'101\\', [\"\\'re\", \\'with\\']]',\n",
       " \"['P', '1101', ['to', 'get', 'of']]\",\n",
       " \"['P', '101', ['money', 'to']]\",\n",
       " \"['P', '1101', ['i', 'was', 'to']]\",\n",
       " \"['P', '1101', ['not', 'a', 'of']]\",\n",
       " \"['P', '101', ['no', 'at']]\",\n",
       " '[\\'P\\', \\'101\\', [\"n\\'t\", \\'him\\']]',\n",
       " '[\\'P\\', \\'101\\', [\"\\'m\", \\'the\\']]',\n",
       " '[\\'P\\', \\'1101\\', [\\'i\\', \"\\'m\", \\'the\\']]',\n",
       " \"['P', '101', ['those', 'and']]\",\n",
       " \"['P', '1101', ['think', 'you', 'the']]\",\n",
       " \"['P', '101', ['you', 'this']]\",\n",
       " \"['P', '1101', ['the', 'same', 'as']]\",\n",
       " \"['P', '101', ['still', 'to']]\",\n",
       " \"['P', '101', ['go', 'the']]\",\n",
       " \"['P', '101', ['he', 'on']]\",\n",
       " \"['P', '101', ['of', 'who']]\",\n",
       " \"['P', '101', ['or', 'or']]\",\n",
       " \"['P', '1001', ['because', 'is']]\",\n",
       " \"['P', '101', ['also', 'the']]\",\n",
       " \"['P', '1001', ['a', 'if']]\",\n",
       " \"['P', '1101', ['as', 'a', 'of']]\",\n",
       " \"['P', '101', ['that', 'that']]\",\n",
       " \"['P', '101', ['what', 'when']]\",\n",
       " \"['P', '1001', ['a', 'but']]\",\n",
       " \"['P', '101', ['the', 'than']]\",\n",
       " \"['P', '1101', ['all', 'the', 'and']]\",\n",
       " \"['P', '101', ['who', 'about']]\",\n",
       " \"['P', '101', ['i', 'think']]\",\n",
       " \"['P', '101', ['hillary', 'the']]\",\n",
       " \"['P', '101', ['but', 'are']]\",\n",
       " \"['P', '1001', ['but', 'are']]\",\n",
       " \"['P', '101', ['a', 'so']]\",\n",
       " \"['P', '101', ['would', 'be']]\",\n",
       " \"['P', '1001', ['to', 'up']]\",\n",
       " \"['P', '101', ['what', 'do']]\",\n",
       " \"['P', '101', ['more', 'for']]\",\n",
       " \"['P', '101', ['we', 'them']]\",\n",
       " \"['P', '1001', ['most', 'in']]\",\n",
       " \"['P', '11001', ['the', 'most', 'in']]\",\n",
       " '[\\'P\\', \\'101\\', [\\'you\\', \"n\\'t\"]]',\n",
       " \"['P', '101', ['not', 'what']]\",\n",
       " \"['P', '101', ['at', 'he']]\",\n",
       " \"['P', '101', ['a', 'a']]\",\n",
       " \"['P', '101', ['just', 'about']]\",\n",
       " '[\\'P\\', \\'101\\', [\\'so\\', \"\\'s\"]]',\n",
       " \"['P', '101', ['it', 'you']]\",\n",
       " '[\\'P\\', \\'101\\', [\"n\\'t\", \\'your\\']]',\n",
       " \"['P', '101', ['not', 'you']]\",\n",
       " \"['P', '101', ['just', 'her']]\",\n",
       " \"['P', '1101', ['of', 'the', 'party']]\",\n",
       " \"['P', '101', ['but', 'it']]\",\n",
       " \"['P', '1001', ['to', 'at']]\",\n",
       " \"['P', '101', ['any', 'now']]\",\n",
       " \"['P', '101', ['who', 'with']]\",\n",
       " \"['P', '101', ['be', 'or']]\",\n",
       " \"['P', '101', ['good', 'to']]\",\n",
       " \"['P', '101', ['to', 'those']]\",\n",
       " '[\\'P\\', \\'1101\\', [\\'you\\', \"\\'re\", \\'to\\']]',\n",
       " \"['P', '101', ['same', 'of']]\",\n",
       " \"['P', '1101', ['the', 'same', 'of']]\",\n",
       " '[\\'P\\', \\'101\\', [\"\\'ll\", \\'you\\']]',\n",
       " '[\\'P\\', \\'1101\\', [\\'i\\', \"\\'ll\", \\'you\\']]',\n",
       " \"['P', '101', ['same', 'that']]\",\n",
       " \"['P', '1101', ['the', 'same', 'that']]\",\n",
       " \"['P', '101', ['i', 'up']]\",\n",
       " \"['P', '101', ['of', 'i']]\",\n",
       " '[\\'P\\', \\'101\\', [\"\\'m\", \\'of\\']]',\n",
       " '[\\'P\\', \\'1101\\', [\\'i\\', \"\\'m\", \\'of\\']]',\n",
       " \"['P', '101', ['just', 'your']]\",\n",
       " \"['P', '1101', ['and', 'the', 'to']]\",\n",
       " '[\\'P\\', \\'101\\', [\"\\'re\", \\'and\\']]',\n",
       " \"['P', '101', ['a', 'good']]\",\n",
       " \"['P', '1001', ['your', 'is']]\",\n",
       " \"['P', '101', ['will', 'all']]\",\n",
       " \"['P', '101', ['how', 'you']]\",\n",
       " \"['P', '101', ['as', 'who']]\",\n",
       " \"['P', '101', ['our', 'in']]\",\n",
       " '[\\'P\\', \\'101\\', [\"\\'m\", \\'this\\']]',\n",
       " '[\\'P\\', \\'1101\\', [\\'i\\', \"\\'m\", \\'this\\']]',\n",
       " \"['P', '101', ['i', 'your']]\",\n",
       " \"['P', '101', ['is', 'up']]\",\n",
       " '[\\'P\\', \\'1101\\', [\\'that\\', \"\\'s\", \\'what\\']]',\n",
       " \"['P', '101', ['would', 'it']]\",\n",
       " '[\\'P\\', \\'101\\', [\"\\'s\", \\'because\\']]',\n",
       " \"['P', '1001', ['our', 'and']]\",\n",
       " \"['P', '101', ['actually', 'to']]\",\n",
       " \"['P', '1001', ['a', 'because']]\",\n",
       " '[\\'P\\', \\'101\\', [\"\\'s\", \\'this\\']]',\n",
       " \"['P', '101', ['or', 'it']]\",\n",
       " \"['P', '101', ['to', 'who']]\",\n",
       " \"['P', '101', ['so', 'they']]\",\n",
       " \"['P', '1001', ['was', 'and']]\",\n",
       " \"['P', '101', ['from', 'the']]\",\n",
       " \"['P', '101', ['and', 'our']]\",\n",
       " \"['P', '1101', ['so', 'it', 'be']]\",\n",
       " \"['P', '101', ['what', 'are']]\",\n",
       " '[\\'P\\', \\'1101\\', [\\'it\\', \"\\'s\", \\'because\\']]',\n",
       " \"['P', '101', ['need', 'to']]\",\n",
       " \"['P', '101', ['i', 'obama']]\",\n",
       " \"['P', '101', ['will', 'us']]\",\n",
       " \"['P', '101', ['because', 'are']]\",\n",
       " \"['P', '10001', ['i', 'that']]\",\n",
       " \"['P', '101', ['of', 'we']]\"]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pattern_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_train.shape[1] # Make sure to replace this with Artur's stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_words_cols = [x for x in np.load('../data/features/ngram_words_features.npy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_pos_cols = [x for x in np.load('../data/features/ngram_pos_features.npy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pointedness_cols + synset_cols + frequency_cols + sentiment_cols + pattern_cols + ngram_words_cols + ngram_pos_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/features/all_features_arr.npy', all_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
